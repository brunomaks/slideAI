{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_landmarks(landmarks, handedness):\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Translate so that wrist is at origin\n",
    "    wrist = landmarks[0]\n",
    "    landmarks = landmarks - wrist\n",
    "\n",
    "    # Scale so that distance between wrist and middle finger MCP is 1\n",
    "    mcp_index = 9  # Middle finger MCP landmark index\n",
    "    scale = np.linalg.norm(landmarks[mcp_index]) # euclidean distance from the origin (wrist)\n",
    "    if scale > 0:\n",
    "        landmarks = landmarks / scale\n",
    "    \n",
    "    # Mirror left hands\n",
    "    if handedness == \"Left\":\n",
    "        landmarks[:, 0]  =  -landmarks[:, 0]\n",
    "\n",
    "    # rotate all landmarks to point up\n",
    "    rotated_landmarks = normalize_rotation(landmarks)\n",
    "\n",
    "    return rotated_landmarks.tolist()\n",
    "\n",
    "def normalize_rotation(landmarks):\n",
    "    # Reference vector: from wrist (now at origin) to middle finger MCP\n",
    "    reference_vector = landmarks[9]  # Middle finger MCP (wrist is at origin)\n",
    "    \n",
    "    # Current angle of reference vector\n",
    "    current_angle = np.arctan2(reference_vector[1], reference_vector[0])\n",
    "    \n",
    "    # Target angle (pointing up in image coordinates = -90 degrees = -pi/2)\n",
    "    # Note: In image coordinates, Y increases downward, so \"up\" is negative Y\n",
    "    target_angle = -np.pi / 2\n",
    "    \n",
    "    # Calculate rotation needed\n",
    "    rotation_angle = target_angle - current_angle\n",
    "    \n",
    "    # Apply rotation\n",
    "    cos_a = np.cos(rotation_angle)\n",
    "    sin_a = np.sin(rotation_angle)\n",
    "    rotation_matrix = np.array([\n",
    "        [cos_a, -sin_a],\n",
    "        [sin_a, cos_a]\n",
    "    ])\n",
    "    \n",
    "    rotated_landmarks = (rotation_matrix @ landmarks.T).T\n",
    "    \n",
    "    return rotated_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6e3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_direction(landmark_list):\n",
    "    wrist = np.array(landmark_list[0])\n",
    "    index_mcp = np.array(landmark_list[5])\n",
    "    middle_mcp = np.array(landmark_list[9])\n",
    "    index_tip = np.array(landmark_list[8])\n",
    "    middle_tip = np.array(landmark_list[12])\n",
    "\n",
    "    palm_center = (wrist + index_mcp + middle_mcp) / 3\n",
    "\n",
    "    finger_tip_avg = (index_tip + middle_tip) / 2\n",
    "    finger_dir = finger_tip_avg - palm_center\n",
    "\n",
    "    angle = math.atan2(finger_dir[1], finger_dir[0])  # angle in radians\n",
    "    return math.degrees(angle)  # convert to degrees\n",
    "\n",
    "def get_direction(angle_degrees):\n",
    "    # Normalize angle to [-180, 180]\n",
    "    angle = ((angle_degrees + 180) % 360) - 180\n",
    "    \n",
    "    if -45 <= angle <= 45:\n",
    "        return \"Right\"\n",
    "    elif angle >= 135 or angle <= -135:\n",
    "        return \"Left\"\n",
    "    elif 45 < angle < 135:\n",
    "        return \"Down\"\n",
    "    else:\n",
    "        return \"Up\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effef649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "DETECTOR_PATH = 'utils/mediapipe_cropper/hand_landmarker.task'\n",
    "MODEL_PATH = 'shared_artifacts/models/gesture_model_20251221_184630.keras'\n",
    "\n",
    "class_names = ['like', 'stop', 'two_up']\n",
    "\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "\n",
    "options = HandLandmarkerOptions(base_options=BaseOptions(model_asset_path=str(DETECTOR_PATH)),\n",
    "                                num_hands=1,\n",
    "                                running_mode=VisionRunningMode.VIDEO)\n",
    "\n",
    "\n",
    "with HandLandmarker.create_from_options(options) as landmarker:\n",
    "    # Open default camera (0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        exit()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "\n",
    "        timestamp_ms = int(time.time() * 1000)\n",
    "\n",
    "        results = landmarker.detect_for_video(mp_image, timestamp_ms)\n",
    "\n",
    "        if results.hand_landmarks:\n",
    "            landmarks = results.hand_landmarks[0]\n",
    "            handedness_category = results.handedness[0][0]\n",
    "            handedness = handedness_category.category_name\n",
    "            confidence = handedness_category.score\n",
    "\n",
    "            print(handedness)\n",
    "\n",
    "            landmark_list = []\n",
    "            for lm in landmarks:\n",
    "                landmark_list.append([lm.x, lm.y])\n",
    "\n",
    "            # angle = compute_direction(landmark_list)\n",
    "            # direction = get_direction(angle)\n",
    "\n",
    "            # double check if not messed up here\n",
    "            # if direction == \"Right\":\n",
    "            #     print(\"Predicted: swipe_right; Confidence: pretty high\")\n",
    "            # elif direction == \"Left\":\n",
    "            #     print(\"Predicted: swipe_left; Confidence: pretty high\")\n",
    "            # else:\n",
    "            normalized_landmarks = normalize_landmarks(landmark_list, handedness)\n",
    "\n",
    "            input_vector = np.array(normalized_landmarks, dtype=np.float32).flatten() # (42,)\n",
    "\n",
    "            input_vector = np.expand_dims(input_vector, axis=0) # (1, 42)\n",
    "\n",
    "            predictions = model.predict(input_vector, verbose=0)\n",
    "\n",
    "            predicted_idx = np.argmax(predictions[0])\n",
    "            confidence = predictions[0][predicted_idx]\n",
    "\n",
    "            predicted_gesture = class_names[predicted_idx]\n",
    "\n",
    "            print(f\"Predicted: {predicted_gesture}; Confidence: {confidence}\")\n",
    "\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
