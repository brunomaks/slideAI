services:
  ml-training-gpu:
      build:
        context: ./ml_service
        dockerfile: Dockerfile.gpu
      container_name: ml-training
      volumes:
        - ./ml_service:/workspace
        - ./shared_artifacts/models:/models
        - ./shared_artifacts/data:/data
        - ./shared_artifacts/images:/images
      environment:
        - MODEL_OUTPUT_PATH=/models
      command: python src/train.py --epochs 10 --set-active

  ml-training-cpu:
    build:
      context: ./ml_service
      dockerfile: Dockerfile.cpu
    container_name: ml-training-cpu
    volumes:
      - ./ml_service:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - CUDA_VISIBLE_DEVICES=-1
      - MODEL_OUTPUT_PATH=/models
    command: python src/train.py --epochs 10 --set-active

  ml-inference:
    build: ./inference
    container_name: ml-inference
    volumes:
      - ./shared_artifacts/models:/models
    environment:
      - MODEL_PATH=/models

  web:
    build: ./web_app
    container_name: web
    volumes:
      - ./web_app:/app
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./web_app/media:/app/media
    ports:
      - "8001:8001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.development
      - MODEL_PATH=/models
      - DATABASE_PATH=/data/database.sqlite
      - ALLOWED_HOSTS=localhost,frontend
      - INFERENCE_URL=http://ml-inference:8002/inference/
      - RESIZE_URL=http://127.0.0.1:8001/resize/
      - HOST_URL=http://127.0.0.1:8001/
      - DEBUG_SAVE=False
    depends_on:
      - ml-inference

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - CHOKIDAR_USEPOLLING=true
