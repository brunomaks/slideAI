services:
  ml-training-landmarks:
    build: ./ml_service_landmarks
    container_name: ml-training-landmarks
    volumes:
      - ./ml_service_landmarks:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - MODEL_OUTPUT_PATH=/models
      - PORT=8003
    ports:
      - "8003:8003"
    profiles: ["gpu"]


  # ML Training Service (runs the Training API server)
  ml-training:
    build: ./ml_service
    container_name: ml-training
    volumes:
      - ./ml_service:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - MODEL_OUTPUT_PATH=/models
      - PORT=8003
    ports:
      - "8003:8003"
    profiles: ["gpu"]
    # GPU support is optional via docker-compose.override.yml

  # CPU-only training service for systems without GPU
  ml-training-cpu:
    build: ./ml_service
    container_name: ml-training
    volumes:
      - ./ml_service:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - CUDA_VISIBLE_DEVICES=-1
      - MODEL_OUTPUT_PATH=/models
      - PORT=8003
    ports:
      - "8003:8003"
    profiles: ["cpu"]
    # Runs the API server (no GPU available)

  ml-inference:
    build: ./inference
    container_name: ml-inference
    volumes:
      - ./shared_artifacts/models:/models
    environment:
      - MODEL_PATH=/models

  web:
    build: ./web_app
    container_name: web
    volumes:
      - ./web_app:/app
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    ports:
      - "8001:8001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.development
      - MODEL_PATH=/models
      - DATABASE_PATH=/data/database.sqlite
      - MEDIA_ROOT=/images
      - ALLOWED_HOSTS=localhost,frontend
      - INFERENCE_URL=http://ml-inference:8002/inference/
      - RESIZE_URL=http://127.0.0.1:8001/resize/
      - HOST_URL=http://127.0.0.1:8001/
      - DEBUG_SAVE=False
      - ML_TRAINING_API_URL=http://ml-training:8003

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - CHOKIDAR_USEPOLLING=true
