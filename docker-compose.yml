services:
  ml-training-landmarks:
    build: ./ml_service_landmarks
    container_name: ml-training-landmarks
    volumes:
      - ./ml_service_landmarks:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/images:/images
      - ./shared_artifacts/data:/data
    environment:
      - MODEL_OUTPUT_PATH=/models
      - DATABASE_PATH=/data/landmarks.sqlite
      - RAW_IMAGES_PATH=/images
      - LANDMARK_DETECTOR_PATH=/models/hand_landmarker.task
      - INFERENCE_API_URL=http://ml-inference-landmarks:8000
      - PORT=8003
      - PYTHONUNBUFFERED=1
    ports:
      - "8003:8003"
    # profiles: [ "gpu" ]

  ml-training:
    build:
      context: ./ml_service
      target: dev
    container_name: ml-training
    volumes:
      - ./ml_service:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - MODEL_OUTPUT_PATH=/models
      - PORT=8003
    ports:
      - "8003:8003"
    profiles: [ "gpu" ]
    # GPU support is optional via docker-compose.override.yml

  ml-training-cpu:
    build:
      context: ./ml_service
      target: dev
    container_name: ml-training
    volumes:
      - ./ml_service:/workspace
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    environment:
      - CUDA_VISIBLE_DEVICES=-1
      - MODEL_OUTPUT_PATH=/models
      - PORT=8003
    ports:
      - "8003:8003"
    profiles: [ "cpu" ]
    # Runs the API server (no GPU available)

  ml-inference-landmarks:
    build: ./ml_inference_landmarks
    container_name: ml-inference-landmarks
    volumes:
      - ./shared_artifacts/models:/models
    environment:
      - ACTIVE_MODEL_PATH=/models/active_model.json
      - MODEL_PATH=/models
      - PYTHONUNBUFFERED=1


  ml-inference:
    build:
      context: ./inference
      target: dev
    container_name: ml-inference
    volumes:
      - ./shared_artifacts/models:/models
      - ./inference:/app
    environment:
      - MODEL_PATH=/models

  web:
    build:
      context: ./web_app
      target: dev
    container_name: web
    volumes:
      - ./web_app:/app
      - ./shared_artifacts/models:/models
      - ./shared_artifacts/data:/data
      - ./shared_artifacts/images:/images
    ports:
      - "8001:8001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.development
      - MODEL_PATH=/models
      - DATABASE_PATH=/data/database.sqlite
      - LANDMARKS_DB_PATH=/data/landmarks.sqlite
      - MEDIA_ROOT=/images
      - ALLOWED_HOSTS=localhost,frontend
      - INFERENCE_URL=http://ml-inference-landmarks:8002/inference
      - HOST_URL=http://127.0.0.1:8001/
      - DEBUG_SAVE=False
      - ML_TRAINING_API_URL=http://ml-training-landmarks:8003
      - PYTHONUNBUFFERED=1

  frontend:
    build:
      context: ./frontend
      target: dev
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true
      - VITE_WS_VIDEO_URL=ws://127.0.0.1:8001/ws/video/
      - VITE_MAX_STREAM_FPS=5
      - VITE_FRAME_QUALITY=0.7
      - VITE_MAX_STREAM_WIDTH=1280
      - VITE_MAX_STREAM_HEIGHT=720
