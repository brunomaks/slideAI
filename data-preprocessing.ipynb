{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e31e21d",
   "metadata": {},
   "source": [
    "# 1) Cropping with mediapipe\n",
    "We start our dataset preprocessing by extracting and cropping hand gestures from the dataset explored in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3df1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k\\like\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [02:11<00:00, 22.74img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k\\stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [02:28<00:00, 20.24img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k\\two_up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6000/6000 [04:44<00:00, 21.12img/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.mediapipe_cropper.cropper import process_images\n",
    "process_images('shared_artifacts/images/hagrid_30k', 'shared_artifacts/images/hagrid_30k_cropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08d132-6efa-4250-bada-558c15e79168",
   "metadata": {},
   "source": [
    "When we have cropped hand gesture images saved, they are all of different sizes. We want them to have an optimal size so that it is: \n",
    "1. Big enough to capture important hand-shape details\n",
    "2. Small enough to train fast\n",
    "3. Consistent across the dataset\n",
    "    \n",
    "So we chose to look at the distribution of all our image dimensions and choose a size close to the *median* or *mean*, then round to a CNN-friendly size (like 64, 96, 128). \n",
    "- *Motivation to this is:* Powers of 2 and Divisibility - many of these numbers are powers of 2 (64, 128, 256, which is close to 224 in practical terms) or easily divisible by 32. This is crucial because standard CNN architectures use multiple layers of pooling operations that typically reduce the image dimensions by half at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a9c5d6-39f2-4c47-9bf2-92e37b0b96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a18e61c-8919-4392-9608-b42df934addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean width: 174 px\n",
      "Mean height: 293 px\n",
      "Median width: 167.0 px\n",
      "Median height: 286.0 px\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"shared_artifacts/images/hagrid_30k_cropped\"\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for label in os.listdir(input_folder):\n",
    "    label_folder = os.path.join(input_folder, label)\n",
    "\n",
    "    for f in os.listdir(label_folder):\n",
    "        img = cv2.imread(os.path.join(label_folder, f))\n",
    "        h, w = img.shape[:2]\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "print(f\"Mean width: {np.mean(widths):.0f} px\")\n",
    "print(f\"Mean height: {np.mean(heights):.0f} px\")\n",
    "print(f\"Median width: {np.median(widths)} px\")\n",
    "print(f\"Median height: {np.median(heights)} px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a3f2f-9cbb-40a8-bcf0-10505ad1197c",
   "metadata": {},
   "source": [
    "# 2) Resizing and Padding\n",
    "So dataset (cropped images of \"swipe\" hand gestures with margin 20px) has:\n",
    "- Width ~ 75 px\n",
    "- Height ~ 125 px\n",
    "\n",
    "Which indicates that:\n",
    "- the images are not square\n",
    "- The aspect ratio is roughly 3:5 (75:125 ≈ 0.6)\n",
    "\n",
    "Since the cropped images are naturally rectangular, if we resize directly to square dimensions like:\n",
    "- 96×96 -> hands will get squashed\n",
    "- 128×128 -> same distortion problem\n",
    "\n",
    "Therefore, we decided to resize while preserving aspect ratio, then pad to a square (add plack pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc2ebe4-12e4-4500-bfb5-7dc5072b99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k_cropped\\like\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2765/2765 [00:04<00:00, 614.14img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k_cropped\\stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2857/2857 [00:04<00:00, 638.18img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: shared_artifacts/images/hagrid_30k_cropped\\two_up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5326/5326 [00:08<00:00, 654.36img/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.resizer.resizer import process_images\n",
    "\n",
    "input_path = \"shared_artifacts/images/hagrid_30k_cropped\"\n",
    "output_path = \"shared_artifacts/images/hagrid_30k_resized\" \n",
    "\n",
    "TARGET_SIZE = 96\n",
    "\n",
    "process_images(input_path, TARGET_SIZE, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974ede6-731e-4274-9620-2ce695b69f1b",
   "metadata": {},
   "source": [
    "# 3) No preprocessing pipeline is perfect - Manual check is still needed!\n",
    "Our utomated filters removed ~97% of problematic images. But those ~3% that are left will be treated by the CNN model will as “truth”, so bad samples → bad model. We don't want that, thus, we perform a quick manual check to remove the last few errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08c6af-2ad8-4a90-8dad-94f0bd78f5b4",
   "metadata": {},
   "source": [
    "## * 3.1) Special case\n",
    "Since \"hagrid\" dataset do not include any gestures that would fit well for the functionality of \"Slide Right\" and \"Slide Left\", we have to create two new gestures. The plan is:\n",
    "- Take all \"two fingers point up\" images.\n",
    "- Rotate them:\n",
    "    - 90° clockwise → becomes Slide Right\n",
    "    - 90° counterclockwise → becomes Slide Left\n",
    "- But because images include left and right hands, rotating alone flips the semantics.\n",
    "- So we have to mirror left-hand images before rotating them to get a clean \"Slide Right\" dataset, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b0f9b-0633-4b2d-b00c-c257e0174c4a",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cec9d-d4b2-4aaf-8ed0-000be56db1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = 'shared_artifacts/images/hagrid_30k_resized/train_val_two_up'\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "img = cv2.imread(os.path.join(path, files[0]))\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Rotate 90 degrees clockwise\n",
    "rotated_cw = cv2.rotate(img_rgb, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# Rotate 90 degrees counter-clockwise\n",
    "rotated_ccw = cv2.rotate(img_rgb, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(rotated_cw)\n",
    "axes[1].set_title('Rotated 90° CW - CORRECT for \"Swipe Right\"')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(rotated_ccw)\n",
    "axes[2].set_title('Rotated 90° CCW - INCORRECT for \"Swipe Left\"')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67deb45-755a-4926-8b81-7d64312624a4",
   "metadata": {},
   "source": [
    "How it should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b8133-d351-4da7-9524-c69c5370248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_horizontal = cv2.flip(img_rgb, 1)  # 1 - horizontal flip\n",
    "rotated_ccw = cv2.rotate(img_horizontal, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_horizontal)\n",
    "axes[1].set_title('Mirrored')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(img_horizontal)\n",
    "axes[2].set_title('Rotated 90° CCW - CORRECT for \"Swipe Left\"')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e67761-7936-4554-ab13-36e17226b1f8",
   "metadata": {},
   "source": [
    "## 3.2) Mirroring and Rotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b2603a-ae86-407d-b893-59f9c51d8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic slide gestures:\n",
      "Done slide gestures.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_path = \"shared_artifacts/images/hagrid_30k_resized/two_up\"\n",
    "output_path_left = \"shared_artifacts/images/hagrid_30k_resized/swipe_left\"\n",
    "output_path_right = \"shared_artifacts/images/hagrid_30k_resized/swipe_right\"\n",
    "\n",
    "Path(output_path_left).mkdir(parents=True, exist_ok=True)\n",
    "Path(output_path_right).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_slide_right(img, filename):\n",
    "    \"\"\"Create Slide Right gesture.\"\"\"\n",
    "    \n",
    "    if \"left\" in filename:\n",
    "        img = cv2.flip(img, 1)  # convert left hand → right hand\n",
    "    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_slide_left(img, filename):\n",
    "    \"\"\"Create Slide Left gesture.\"\"\"\n",
    "    \n",
    "    if \"right\" in filename:\n",
    "        img = cv2.flip(img, 1) # convert right hand → left hand\n",
    "    img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    return img\n",
    "    \n",
    "\n",
    "print(\"Generating synthetic slide gestures:\")\n",
    "\n",
    "for file in os.listdir(input_path):\n",
    "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(input_path, file)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\" Could not read image: {file}\")\n",
    "        continue\n",
    "\n",
    "    # SLIDE RIGHT\n",
    "    slide_right_img = process_slide_right(img.copy(), file)\n",
    "    right_output_path = os.path.join(output_path_right, f\"right_{file}\")\n",
    "    cv2.imwrite(right_output_path, slide_right_img)\n",
    "\n",
    "    # SLIDE LEFT\n",
    "    slide_left_img = process_slide_left(img.copy(), file)\n",
    "    left_output_path = os.path.join(output_path_left, f\"left_{file}\")\n",
    "    cv2.imwrite(left_output_path, slide_left_img)\n",
    "\n",
    "print(\"Done slide gestures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72156c1",
   "metadata": {},
   "source": [
    "# 4) Train/test split\n",
    "After having cropped and resized the dataset, we need to split the dataset into \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b18e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 2765 images\n",
      " - Moving 414 images to test set\n",
      " - Moved 414/414 images to the test set\n",
      "Label : 2857 images\n",
      " - Moving 428 images to test set\n",
      " - Moved 428/428 images to the test set\n",
      "Label : 3000 images\n",
      " - Moving 450 images to test set\n",
      " - Moved 450/450 images to the test set\n",
      "Label t: 3000 images\n",
      " - Moving 450 images to test set\n",
      " - Moved 450/450 images to the test set\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "TEST_SET_PERCENTAGE = 0.15\n",
    "\n",
    "input_path = \"shared_artifacts/images/hagrid_30k_resized\"\n",
    "output_path = \"shared_artifacts/images/hagrid_30k_test\"\n",
    "\n",
    "labels = [l for l in os.listdir(input_path) if os.path.isdir(os.path.join(input_path, l))]\n",
    "\n",
    "for label in labels:\n",
    "    label_input_path = os.path.join(input_path, label)\n",
    "    label_output_path = os.path.join(output_path, label)\n",
    "\n",
    "    os.makedirs(label_output_path, exist_ok=True) # make sure the dir exists\n",
    "\n",
    "    files = [f for f in os.listdir(label_input_path)]\n",
    "    print(f\"Label {label[10:]}: {len(files)} images\")\n",
    "\n",
    "    image_count = int(len(files) * TEST_SET_PERCENTAGE)\n",
    "    print(f\" - Moving {image_count} images to test set\")\n",
    "\n",
    "    files_to_move = random.sample(files, image_count)\n",
    "\n",
    "    moved_count = 0\n",
    "\n",
    "    for file in files_to_move:\n",
    "        src_path = os.path.join(label_input_path, file)\n",
    "        dst_path = os.path.join(label_output_path, file)\n",
    "\n",
    "        try:\n",
    "            shutil.move(src_path, dst_path)\n",
    "            moved_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving {src_path}: {e}\")\n",
    "    \n",
    "    print(f\" - Moved {moved_count}/{image_count} images to the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d7b32",
   "metadata": {},
   "source": [
    "Rename the directories to follow the conventional naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "306a4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"shared_artifacts/images/train\"\n",
    "test = \"shared_artifacts/images/test\"\n",
    "\n",
    "os.rename(\"shared_artifacts/images/hagrid_30k_resized\", train)\n",
    "os.rename(\"shared_artifacts/images/hagrid_30k_test\", test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1203cae",
   "metadata": {},
   "source": [
    "Lets take a final look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae63c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_artifacts/images/train - Label : 2351 images\n",
      "shared_artifacts/images/train - Label : 2429 images\n",
      "shared_artifacts/images/train - Label : 2550 images\n",
      "shared_artifacts/images/train - Label t: 2550 images\n",
      "shared_artifacts/images/test - Label : 414 images\n",
      "shared_artifacts/images/test - Label : 428 images\n",
      "shared_artifacts/images/test - Label : 450 images\n",
      "shared_artifacts/images/test - Label t: 450 images\n"
     ]
    }
   ],
   "source": [
    "for dir in [train, test]:\n",
    "    total_images = 0\n",
    "    for label in os.listdir(dir):\n",
    "        label_folder = os.path.join(dir, label)\n",
    "        num_images = len(os.listdir(label_folder))\n",
    "        print(f\"{dir} - Label {label[10:]}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66e41b-db5c-48c3-b3eb-e770cc8a3b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
