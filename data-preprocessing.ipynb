{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e31e21d",
   "metadata": {},
   "source": [
    "\n",
    "We start our dataset preprocessing by extracting and cropping hand gestures from the dataset explored in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mediapipe_cropper.cropper import process_images\n",
    "process_images('shared_artifacts/images/hagrid_30k', 'shared_artifacts/images/hagrid_30k_cropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08d132-6efa-4250-bada-558c15e79168",
   "metadata": {},
   "source": [
    "When we have cropped hand gesture images saved, they are all of different sizes. We want them to have an optimal size so that it is: \n",
    "1. Big enough to capture important hand-shape details\n",
    "2. Small enough to train fast\n",
    "3. Consistent across the dataset\n",
    "    \n",
    "So we chose to look at the distribution of all our image dimensions and choose a size close to the *median* or *mean*, then round to a CNN-friendly size (like 64, 96, 128). \n",
    "- *Motivation to this is:* Powers of 2 and Divisibility - many of these numbers are powers of 2 (64, 128, 256, which is close to 224 in practical terms) or easily divisible by 32. This is crucial because standard CNN architectures use multiple layers of pooling operations that typically reduce the image dimensions by half at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a18e61c-8919-4392-9608-b42df934addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean width: 86 px\n",
      "Mean height: 116 px\n",
      "Median width: 82.0 px\n",
      "Median height: 108.5 px\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "input_folder = \"shared_artifacts/images/hagrid_30k_cropped\"\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for label in os.listdir(input_folder):\n",
    "    label_folder = os.path.join(input_folder, label)\n",
    "\n",
    "    for f in os.listdir(label_folder):\n",
    "        img = cv2.imread(os.path.join(label_folder, f))\n",
    "        h, w = img.shape[:2]\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "print(f\"Mean width: {np.mean(widths):.0f} px\")\n",
    "print(f\"Mean height: {np.mean(heights):.0f} px\")\n",
    "print(f\"Median width: {np.median(widths)} px\")\n",
    "print(f\"Median height: {np.median(heights)} px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a3f2f-9cbb-40a8-bcf0-10505ad1197c",
   "metadata": {},
   "source": [
    "#### So dataset (cropped images of \"swipe\" hand gestures with margin 20px) has:\n",
    "- Width ~ 75 px\n",
    "- Height ~ 125 px\n",
    "\n",
    "Which indicates that:\n",
    "- the images are not square\n",
    "- The aspect ratio is roughly 3:5 (75:125 ≈ 0.6)\n",
    "\n",
    "Since the cropped images are naturally rectangular, if we resize directly to square dimensions like:\n",
    "- 96×96 -> hands will get squashed\n",
    "- 128×128 -> same distortion problem\n",
    "\n",
    "Therefore, we decided to resize while preserving aspect ratio, then pad to a square (add plack pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2ebe4-12e4-4500-bfb5-7dc5072b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.resizer.resizer import process_images\n",
    "\n",
    "input_path = \"shared_artifacts/images/hagrid_30k_cropped\"\n",
    "output_path = \"shared_artifacts/images/hagrid_30k_resized\" \n",
    "\n",
    "TARGET_SIZE = 94\n",
    "\n",
    "process_images(input_path, TARGET_SIZE, TARGET_SIZE, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72156c1",
   "metadata": {},
   "source": [
    "After having cropped and resized the dataset, we need to split the dataset into \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label like: 1235 images\n",
      " - Moving 185 images to test set\n",
      " - Moved 185/185 images to test set\n",
      "Label stop: 1359 images\n",
      " - Moving 203 images to test set\n",
      " - Moved 203/203 images to test set\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "TEST_SET_PERCENTAGE = 0.15\n",
    "\n",
    "input_path = \"shared_artifacts/images/hagrid_30k_resized\"\n",
    "output_path = \"shared_artifacts/images/hagrid_30k_test\"\n",
    "\n",
    "labels = [l for l in os.listdir(input_path) if os.path.isdir(os.path.join(input_path, l))]\n",
    "\n",
    "for label in labels:\n",
    "    label_input_path = os.path.join(input_path, label)\n",
    "    label_output_path = os.path.join(output_path, label)\n",
    "\n",
    "    os.makedirs(label_output_path, exist_ok=True) # make sure the dir exists\n",
    "\n",
    "    files = [f for f in os.listdir(label_input_path)]\n",
    "    print(f\"Label {label[10:]}: {len(files)} images\")\n",
    "\n",
    "    image_count = int(len(files) * TEST_SET_PERCENTAGE)\n",
    "    print(f\" - Moving {image_count} images to test set\")\n",
    "\n",
    "    files_to_move = random.sample(files, image_count)\n",
    "\n",
    "    moved_count = 0\n",
    "\n",
    "    for file in files_to_move:\n",
    "        src_path = os.path.join(label_input_path, file)\n",
    "        dst_path = os.path.join(label_output_path, file)\n",
    "\n",
    "        try:\n",
    "            shutil.move(src_path, dst_path)\n",
    "            moved_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving {src_path}: {e}\")\n",
    "    \n",
    "    print(f\" - Moved {moved_count}/{image_count} images to the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d7b32",
   "metadata": {},
   "source": [
    "Rename the directories to follow the conventional naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"shared_artifacts/images/train\"\n",
    "test = \"shared_artifacts/images/test\"\n",
    "\n",
    "os.rename(\"shared_artifacts/images/hagrid_30k_resized\", train)\n",
    "os.rename(\"shared_artifacts/images/hagrid_30k_test\", test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1203cae",
   "metadata": {},
   "source": [
    "Lets take a final look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae63c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_artifacts/images/train - Label like: 1050 images\n",
      "shared_artifacts/images/train - Label stop: 1156 images\n",
      "shared_artifacts/images/test - Label like: 185 images\n",
      "shared_artifacts/images/test - Label stop: 203 images\n"
     ]
    }
   ],
   "source": [
    "for dir in [train, test]:\n",
    "    total_images = 0\n",
    "    for label in os.listdir(dir):\n",
    "        label_folder = os.path.join(dir, label)\n",
    "        num_images = len(os.listdir(label_folder))\n",
    "        print(f\"{dir} - Label {label[10:]}: {num_images} images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
